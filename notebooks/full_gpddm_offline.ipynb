{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from helpers import build_distance_matrix\n",
    "from macro_models import batched_gaussian_process\n",
    "from priors import diffusion_prior, length_scale_prior\n",
    "from micro_models import dynamic_batch_diffusion, fast_dm_simulate, diffusion_trial\n",
    "from networks_2 import DynamicGaussianNetwork\n",
    "from context import generate_design_matrix\n",
    "from transformations import unscale_z, scale_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('CPU')\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3200\n",
    "N_PARAMS = 6\n",
    "DIST_MAT = build_distance_matrix(T)\n",
    "# AMPLITUDES = [0.20, 0.20, 0.20, 0.20, 0.20, 0.15]\n",
    "AMPLITUDES = [0.15, 0.15, 0.15, 0.15, 0.1, 0.05]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TEST_SIZE = 10\n",
    "N_CHUNKS = 4000\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN = [5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
    "MACRO_STD = [2.8, 2.8, 2.8, 2.8, 2.8, 2.8]\n",
    "\n",
    "MICRO_MEAN = [1.3, 1.3, 1.3, 1.3, 1.3, 0.3]\n",
    "MICRO_STD = [1.0, 1.0, 1.0, 1.0, 0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPIRIC_COLOR = '#1F1F1F'\n",
    "NEURAL_COLOR = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fun(batch_size):\n",
    "    theta0 = diffusion_prior(batch_size, n_cond=N_PARAMS-2)\n",
    "    eta = length_scale_prior(batch_size, N_PARAMS)\n",
    "    theta_t = batched_gaussian_process(theta0, DIST_MAT, eta, amplitudes=AMPLITUDES)\n",
    "    context = generate_design_matrix(batch_size, T)\n",
    "\n",
    "    rt = dynamic_batch_diffusion(theta_t, context)\n",
    "    x = np.concatenate((rt, to_categorical(context[:, :, np.newaxis])), axis=-1)\n",
    "\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    theta_t_z = scale_z(theta_t, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z, theta_t_z, x = generator_fun(10)\n",
    "print(eta_z.shape)\n",
    "print(theta_t_z.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axarr = plt.subplots(10, 6, figsize=(25, 30))\n",
    "# time = np.arange(1, theta_t.shape[1]+1)\n",
    "# for j in range(10):\n",
    "#     for i in range(6):\n",
    "#         ax = axarr[j, i]\n",
    "#         ax.plot(time, theta_t[j, :, i], label='True', color='black', linestyle='dashed')\n",
    "#         sns.despine(ax=ax)\n",
    "#         ax.legend()\n",
    "#         ax.grid(alpha=0.3)\n",
    "# f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presimulate_data():\n",
    "    for n in range(N_CHUNKS):\n",
    "        if n > 261:\n",
    "            eta_z, theta_t_z, x = generator_fun(BATCH_SIZE)\n",
    "            np.save(f'../data/offline_data_new/data/x_{n}.npy', x)\n",
    "            np.save(f'../data/offline_data_new/parameters/eta_params_{n}.npy', eta_z)\n",
    "            np.save(f'../data/offline_data_new/parameters/theta_params_{n}.npy', theta_t_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presimulate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkLoader:\n",
    "    def __init__(self, path_to_data):\n",
    "        self.path_to_data = path_to_data\n",
    "        self.data_list = sorted(os.listdir(os.path.join(path_to_data, 'data')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.eta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/eta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.theta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/theta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.num_batches = len(self.data_list)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_index < self.num_batches:\n",
    "            self.current_index += 1\n",
    "            idx = self.indices[self.current_index -1]\n",
    "            batch_x = np.load(os.path.join(self.path_to_data, 'data', self.data_list[idx]))\n",
    "            batch_eta_params = np.load(os.path.join(self.path_to_data, 'parameters/eta', self.eta_list[idx]))\n",
    "            batch_theta_params = np.load(os.path.join(self.path_to_data, 'parameters/theta', self.theta_list[idx]))\n",
    "            return batch_eta_params, batch_theta_params, batch_x\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ChunkLoader('../data/offline_data_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_gru_units': 32, #256\n",
    "    'embedding_lstm_units' : 32, #256\n",
    "    'dense_pre_args': dict(units=32, activation='selu', kernel_initializer='lecun_normal'), #256\n",
    "    'dense_micro_args': dict(units=32, activation='selu', kernel_initializer='lecun_normal'), #128\n",
    "    'dense_macro_args': dict(units=32, activation='selu', kernel_initializer='lecun_normal'), #128\n",
    "    'macro_lstm_units': 32, #128\n",
    "    'n_micro_params': 6,\n",
    "    'n_macro_params': 6\n",
    "}\n",
    "network = DynamicGaussianNetwork(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=2500,\n",
    "    decay_rate=0.8,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step, batch in enumerate(generator):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            eta_z, theta_t_z, data = batch\n",
    "\n",
    "            idx = np.random.choice(np.arange(0, 16), size=8, replace=False, p=None)\n",
    "\n",
    "            # Forward pass\n",
    "            eta_z_hat, theta_t_z_hat = network(data[idx], eta_z[idx])\n",
    "\n",
    "            # loss computation\n",
    "            loss_eta = nll(eta_z[idx], eta_z_hat)\n",
    "            loss_theta = nll(theta_t_z[idx], theta_t_z_hat)\n",
    "            total_loss = loss_eta + loss_theta\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss.Macro: {:.3f},Loss.Micro: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss_eta.numpy(), loss_theta.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=loader.num_batches, desc=f'Training Epoch {ep}') as p_bar:\n",
    "        loss_ep = epoch_trainer(loader, network, optimizer, steps_per_epoch, p_bar)\n",
    "        losses.append(loss_ep)\n",
    "    network.save_weights('../trained_networks/gp_ddm_3200_joint_offline_factorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights('../trained_networks/gp_ddm_3200_joint_offline_factorized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_test, theta_t_z_test, x_test = generator_fun(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_pred, theta_t_z_pred = network.sample_n(x_test, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_test = unscale_z(eta_z_test, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_test = unscale_z(theta_t_z_test, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "eta_pred = unscale_z(eta_z_pred, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_pred = unscale_z(theta_t_z_pred, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t_pred_mean = theta_t_pred.mean()\n",
    "theta_t_pred_std = theta_t_pred.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualize Predictions Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(TEST_SIZE, 6, figsize=(25, 30))\n",
    "std_mul = 1\n",
    "time = np.arange(1, theta_t_test.shape[1]+1)\n",
    "for j in range(TEST_SIZE):\n",
    "    for i in range(6):\n",
    "        ax = axarr[j, i]\n",
    "        ax.plot(time, theta_t_test[j, :, i], label='True', color='black', linestyle='dashed')\n",
    "        ax.plot(time, theta_t_pred_mean[j, :, i], label='Pred', lw=2, color='#8c6eb5')\n",
    "        ax.fill_between(time, \n",
    "                        theta_t_pred_mean[j, :, i] + std_mul * theta_t_pred_std[j, :, i], \n",
    "                        theta_t_pred_mean[j, :, i] - std_mul * theta_t_pred_std[j, :, i], color='#8c6eb5', alpha=0.3)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "f.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
