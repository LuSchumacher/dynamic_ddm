{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from helpers import build_distance_matrix\n",
    "from macro_models import batched_gaussian_process\n",
    "from priors import diffusion_prior, length_scale_prior\n",
    "from micro_models import dynamic_batch_diffusion, fast_dm_simulate, diffusion_trial\n",
    "from networks_2 import DynamicGaussianNetwork\n",
    "from context import generate_design_matrix\n",
    "from transformations import unscale_z, scale_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('CPU')\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 24.00 GB\n",
      "maxCacheSize: 8.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 10:08:40.064278: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-07 10:08:40.064724: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "T = 3200\n",
    "N_PARAMS = 6\n",
    "DIST_MAT = build_distance_matrix(T)\n",
    "AMPLITUDES = [0.15, 0.15, 0.15, 0.15, 0.1, 0.05]\n",
    "\n",
    "N_SAMPLES  = 2000\n",
    "BATCH_SIZE = 16\n",
    "TEST_SIZE = 10\n",
    "N_CHUNKS = 4000\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN = [5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
    "MACRO_STD = [2.8, 2.8, 2.8, 2.8, 2.8, 2.8]\n",
    "\n",
    "MICRO_MEAN = [1.3, 1.3, 1.3, 1.3, 1.3, 0.3]\n",
    "MICRO_STD = [1.0, 1.0, 1.0, 1.0, 0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPIRIC_COLOR = '#1F1F1F'\n",
    "NEURAL_COLOR = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fun(batch_size):\n",
    "    theta0 = diffusion_prior(batch_size, n_cond=N_PARAMS-2)\n",
    "    eta = length_scale_prior(batch_size, N_PARAMS)\n",
    "    theta_t = batched_gaussian_process(theta0, DIST_MAT, eta, amplitudes=AMPLITUDES)\n",
    "    context = generate_design_matrix(batch_size, T)\n",
    "\n",
    "    rt = dynamic_batch_diffusion(theta_t, context)\n",
    "    x = np.concatenate((rt, to_categorical(context[:, :, np.newaxis])), axis=-1)\n",
    "\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    theta_t_z = scale_z(theta_t, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n",
      "(10, 3200, 6)\n",
      "(10, 3200, 5)\n"
     ]
    }
   ],
   "source": [
    "eta_z, theta_t_z, x = generator_fun(10)\n",
    "print(eta_z.shape)\n",
    "print(theta_t_z.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axarr = plt.subplots(10, 6, figsize=(25, 30))\n",
    "# time = np.arange(1, theta_t.shape[1]+1)\n",
    "# for j in range(10):\n",
    "#     for i in range(6):\n",
    "#         ax = axarr[j, i]\n",
    "#         ax.plot(time, theta_t[j, :, i], label='True', color='black', linestyle='dashed')\n",
    "#         sns.despine(ax=ax)\n",
    "#         ax.legend()\n",
    "#         ax.grid(alpha=0.3)\n",
    "# f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presimulate_data():\n",
    "    for n in range(N_CHUNKS):\n",
    "        if n > 261:\n",
    "            eta_z, theta_t_z, x = generator_fun(BATCH_SIZE)\n",
    "            np.save(f'../data/offline_data_new/data/x_{n}.npy', x)\n",
    "            np.save(f'../data/offline_data_new/parameters/eta_params_{n}.npy', eta_z)\n",
    "            np.save(f'../data/offline_data_new/parameters/theta_params_{n}.npy', theta_t_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presimulate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkLoader:\n",
    "    def __init__(self, path_to_data):\n",
    "        self.path_to_data = path_to_data\n",
    "        self.data_list = sorted(os.listdir(os.path.join(path_to_data, 'data')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.eta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/eta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.theta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/theta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.num_batches = len(self.data_list)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_index < self.num_batches:\n",
    "            self.current_index += 1\n",
    "            idx = self.indices[self.current_index -1]\n",
    "            batch_x = np.load(os.path.join(self.path_to_data, 'data', self.data_list[idx]))\n",
    "            batch_eta_params = np.load(os.path.join(self.path_to_data, 'parameters/eta', self.eta_list[idx]))\n",
    "            batch_theta_params = np.load(os.path.join(self.path_to_data, 'parameters/theta', self.theta_list[idx]))\n",
    "            return batch_eta_params, batch_theta_params, batch_x\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ChunkLoader('../data/offline_data_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_gru_units': 256, #256\n",
    "    'embedding_lstm_units' : 256, #256\n",
    "    'dense_pre_args': dict(units=256, activation='selu', kernel_initializer='lecun_normal'), #256\n",
    "    'dense_micro_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'), #128\n",
    "    'dense_macro_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'), #128\n",
    "    'macro_lstm_units': 128, #128\n",
    "    'n_micro_params': 6,\n",
    "    'n_macro_params': 6\n",
    "}\n",
    "network = DynamicGaussianNetwork(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=2500,\n",
    "    decay_rate=0.8,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step, batch in enumerate(generator):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            eta_z, theta_t_z, data = batch\n",
    "\n",
    "            idx = np.random.choice(np.arange(0, 16), size=8, replace=False, p=None)\n",
    "\n",
    "            # Forward pass\n",
    "            eta_z_hat, theta_t_z_hat = network(data[idx], eta_z[idx])\n",
    "\n",
    "            # loss computation\n",
    "            loss_eta = nll(eta_z[idx], eta_z_hat)\n",
    "            loss_theta = nll(theta_t_z[idx], theta_t_z_hat)\n",
    "            total_loss = loss_eta + loss_theta\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss.Macro: {:.3f},Loss.Micro: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss_eta.numpy(), loss_theta.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=loader.num_batches, desc=f'Training Epoch {ep}') as p_bar:\n",
    "        loss_ep = epoch_trainer(loader, network, optimizer, steps_per_epoch, p_bar)\n",
    "        losses.append(loss_ep)\n",
    "    network.save_weights('../trained_networks/gp_ddm_3200_joint_offline_factorized_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ../trained_networks/gp_ddm_3200_joint_offline_factorized_new",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39m../trained_networks/gp_ddm_3200_joint_offline_factorized_new\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/cogModel/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/cogModel/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     27\u001b[0m error_message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnot found in checkpoint\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFailed to find any \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmatching files for\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSliced checkpoints are not supported\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mData type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msupported\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[1;32m     36\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mUnimplementedError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ../trained_networks/gp_ddm_3200_joint_offline_factorized_new"
     ]
    }
   ],
   "source": [
    "network.load_weights('../trained_networks/gp_ddm_3200_joint_offline_factorized_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_test, theta_t_z_test, x_test = generator_fun(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_pred, theta_t_z_pred = network.sample_n(x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_test = unscale_z(eta_z_test, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_test = unscale_z(theta_t_z_test, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "eta_pred = unscale_z(eta_z_pred, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_pred = unscale_z(theta_t_z_pred, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t_pred_mean = theta_t_pred.numpy().mean(axis=0)\n",
    "theta_t_pred_std = theta_t_pred.numpy().std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(TEST_SIZE, 6, figsize=(25, 30))\n",
    "std_mul = 1\n",
    "time = np.arange(1, theta_t_test.shape[1]+1)\n",
    "for j in range(TEST_SIZE):\n",
    "    for i in range(6):\n",
    "        ax = axarr[j, i]\n",
    "        ax.plot(time, theta_t_test[j, :, i], label='True', color='black', linestyle='dashed')\n",
    "        ax.plot(time, theta_t_pred_mean[j, :, i], label='Pred', lw=2, color='#8c6eb5')\n",
    "        ax.fill_between(time, \n",
    "                        theta_t_pred_mean[j, :, i] + std_mul * theta_t_pred_std[j, :, i], \n",
    "                        theta_t_pred_mean[j, :, i] - std_mul * theta_t_pred_std[j, :, i], color='#8c6eb5', alpha=0.3)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fit on empirical data (Subject 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights('../trained_networks/gp_ddm_3200_joint_offline_factorized_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('../data/data_lexical_decision.csv', sep=',', header=0)\n",
    "\n",
    "person_data = data[data.id == 10]\n",
    "\n",
    "# negative rts for error responses\n",
    "person_data.rt.loc[person_data.acc == 0] = -person_data.rt.loc[person_data.acc == 0]\n",
    "\n",
    "# iterate over subjects\n",
    "x_nn = np.zeros((1, T, 5))\n",
    "\n",
    "rt = np.array([person_data.rt])[:, :, np.newaxis]\n",
    "stim_type = np.array([person_data.stim_type])[:, :, np.newaxis] - 1 \n",
    "context = to_categorical(stim_type)\n",
    "x_nn = tf.concat((rt, context), axis=-1)\n",
    "\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_z, post_theta_t_z = network.sample_n(x_nn, N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fast-dm parameter estimates\n",
    "fast_dm_params = pd.read_csv('../data/parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "fast_dm_params = fast_dm_params.to_numpy()[:, 1:]\n",
    "fast_dm_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with fast_dm for all subjects\n",
    "context = person_data.context.to_numpy() - 1\n",
    "pred_rt_fast_dm = fast_dm_simulate(fast_dm_params[9], context)\n",
    "pred_rt_fast_dm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check(emp_data, post_theta_t, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data.stim_type.values - 1\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data.rt.values)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/n_sim, dtype=np.int32)\n",
    "    theta = post_theta_t[idx]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta[sim, t, context[t]], theta[sim, t, 4], theta[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt, emp_rt, sma_emp_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with neural for all subjects\n",
    "pred_rt_neural = np.zeros((N_SUBS, N_SIM, N_OBS))\n",
    "sma_pred_rt_neural = np.zeros((N_SUBS, N_SIM, N_OBS))\n",
    "\n",
    "pred_rt_quantiles = np.zeros((N_SUBS, 2, N_OBS))\n",
    "pred_rt_medians = np.zeros((N_SUBS, N_OBS))\n",
    "\n",
    "emp_rt =  np.zeros((N_SUBS, N_OBS))\n",
    "sma_emp_rt =  np.zeros((N_SUBS, N_OBS))\n",
    "\n",
    "for sub in range(N_SUBS):\n",
    "    # predict RTs\n",
    "    person_data = data[data.id == sub+1]\n",
    "    pred_rt_neural[sub], sma_pred_rt_neural[sub], emp_rt[sub], sma_emp_rt[sub] = pr_check(person_data, post_theta_t[:, sub, :, :], N_SIM)\n",
    "    # compute RT quantiles\n",
    "    pred_rt_quantiles[sub] = np.quantile(sma_pred_rt_neural[sub], [0.025, 0.975], axis=0)\n",
    "    pred_rt_medians[sub] = np.median(sma_pred_rt_neural[sub], axis=0)\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=800\n",
    "emp_data_horizon = x_nn[:, :N_OBS-horizon, :]\n",
    "\n",
    "# inference on restircted data\n",
    "post_eta_z_horizon = np.zeros((N_SAMPLES, N_SUBS, N_OBS-horizon, N_PARAMS))\n",
    "post_theta_t_z_horizon = np.zeros((N_SAMPLES, N_SUBS, N_OBS-horizon, N_PARAMS))\n",
    "for i in range(len(ids)):\n",
    "    post_eta_z_horizon[:, i:i+1, :, :], post_theta_t_z_horizon[:, i:i+1, :, :] = network.sample_n(emp_data_horizon[i:i+1], N_SAMPLES)\n",
    "    print(\"Sub nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_last = unscale_z(post_eta_z_horizon[:, :, -1, :], MACRO_MEAN, MACRO_STD)\n",
    "post_theta_last = unscale_z(post_theta_t_z_horizon[:, :, -1, :], MICRO_MEANS,  MICRO_STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/N_SIM, dtype=np.int32)\n",
    "post_eta_last_select = post_eta_last[idx]\n",
    "post_theta_last_select = post_theta_last[idx]\n",
    "post_eta_last_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic parameters and simulate RTs\n",
    "pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "sma_pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "\n",
    "context = x_nn[:, :, 1:].argmax(axis=2)[:, T-horizon:]\n",
    "\n",
    "for sub in range(N_SUBS):\n",
    "    for i in range(N_SIM):\n",
    "        pred_theta_t = random_walk(post_theta_last_select[i, sub:sub+1], post_eta_last_select[i, sub:sub+1], horizon)\n",
    "        pred_rt_horizon[i, sub:sub+1] = np.abs(dynamic_batch_diffusion(pred_theta_t, context[sub:sub+1]).astype(np.float32))\n",
    "        sma_pred_rt_horizon[i, sub, :, 0] = talib.SMA(pred_rt_horizon[i, sub, :, 0], timeperiod=5)\n",
    "\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_horizon_medians = np.median(sma_pred_rt_horizon, axis=0)\n",
    "pred_rt_horizon_quantiles = np.quantile(sma_pred_rt_horizon, [0.025, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderLegend2(ax=None,order=None,unique=False):\n",
    "    if ax is None: ax=plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0])) # sort both labels and handles by labels\n",
    "    if order is not None: # Sort according to a given list (not necessarily complete)\n",
    "        keys=dict(zip(order,range(len(order))))\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t,keys=keys: keys.get(t[0],np.inf)))\n",
    "    if unique:  labels, handles= zip(*unique_everseen(zip(labels,handles), key = labels)) # Keep only the first of each handle\n",
    "    ax.legend(handles, labels,\n",
    "              fontsize=16, loc='upper right')\n",
    "    return(handles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "horizon = 800\n",
    "for sub in range(N_SUBS):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                        gridspec_kw={'width_ratios': [6, 1]})\n",
    "    axrr = ax.flat\n",
    "    # plot empiric and predicted response times series\n",
    "    time = np.arange(N_OBS) \n",
    "\n",
    "    axrr[0].plot(time, sma_emp_rt[sub], color=EMPIRIC_COLOR, lw=1.4, alpha=0.8, label='SMA5: Empiric')\n",
    "    axrr[0].plot(time[:N_OBS-horizon], pred_rt_medians[sub, :N_OBS-horizon], color=NEURAL_COLOR, lw=1.4, label='SMA5: Post. re-simulation median', alpha=0.8)\n",
    "    axrr[0].plot(time[N_OBS-horizon:], pred_rt_horizon_medians[sub], color=\"#b35032\", lw=1.4, label='SMA5: Multi-horizon predictive median', alpha=0.6)\n",
    "    axrr[0].fill_between(time[N_OBS-horizon:], pred_rt_horizon_quantiles[0, sub, :, 0], pred_rt_horizon_quantiles[1, sub, :, 0], color=\"#b35032\", linewidth=0, alpha=0.4, label='Multi-horizon predictive 95% CI')\n",
    "    axrr[0].fill_between(time[:N_OBS-horizon], pred_rt_quantiles[sub, 0, :N_OBS-horizon], pred_rt_quantiles[sub, 1, :N_OBS-horizon], color=NEURAL_COLOR, linewidth=0, alpha=0.5, label='Post. re-simulation 95% CI')\n",
    "    \n",
    "    for idx in np.argwhere(person_data.session.diff().values == 1):\n",
    "        if idx == 800:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    for idx in np.argwhere(person_data.block.diff().values == 1):\n",
    "        if idx == 100:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    sns.despine(ax=axrr[0])\n",
    "    axrr[0].set_ylabel('RT(s)', fontsize=18, rotation=0, labelpad=40)\n",
    "    axrr[0].set_xlabel('\\nTrial', fontsize=18)\n",
    "    axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    f.legend(fontsize=16, loc='center', \n",
    "            bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "    axrr[0].grid(False)\n",
    "    axrr[0].set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "    # plot empiric and predicted response time dist\n",
    "    plt.setp(ax, ylim=(0, 1.5))\n",
    "    sns.histplot(y=np.abs(emp_rt[sub]), fill=EMPIRIC_COLOR, color=EMPIRIC_COLOR, alpha=0.8, label=\"Empiric\", ax=axrr[1], stat=\"density\", bins=250, linewidth=0)#062759\n",
    "    sns.kdeplot(y=np.abs(pred_rt_fast_dm[sub]), fill= COMPARISON_COLOR, color=COMPARISON_COLOR, alpha=0.3, label=\"Fast-dm\", ax=axrr[1], linewidth=3.5)#598f70\n",
    "    sns.kdeplot(y=pred_rt_neural[sub].flatten(), fill=NEURAL_COLOR, color=NEURAL_COLOR, alpha=0.3, label=\"Dynamic DDM\", ax=axrr[1], linewidth=3.5)\n",
    "\n",
    "    axrr[1].legend(fontsize=16)\n",
    "    axrr[1].set_xlabel('', fontsize=18)\n",
    "    axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    axrr[1].set_yticklabels('')\n",
    "    axrr[1].set_xticklabels('')\n",
    "    axrr[1].xaxis.set_ticks([])\n",
    "    axrr[1].yaxis.set_ticks([])\n",
    "    axrr[1].get_xaxis().set_visible(False)\n",
    "    for line in axrr[1].get_lines():\n",
    "        line.set_alpha(1)\n",
    "    sns.despine(ax=axrr[1], bottom=True)\n",
    "\n",
    "    axrr[0].annotate('Re-simulation',\n",
    "                xy=(0.38, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].annotate('Prediction',\n",
    "                xy=(0.84, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].tick_params(length=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.05)\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/rt_time_series_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors(dynamic_posterior, fast_dm_params, par_labels, par_names, \n",
    "                            ground_truths=None):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "        \n",
    "    means = dynamic_posterior.mean(axis=0)\n",
    "    # quantiles = np.quantile(dynamic_posterior, [0.025, 0.975], axis=0)\n",
    "    stds = dynamic_posterior.std(axis=0)\n",
    "    \n",
    "    post_max = np.array(means).max(axis=0)\n",
    "    post_min = np.array(means).min(axis=0)\n",
    "    upper_y_ax = post_max + [1, 1, 1, 1, 0.2, 0.05]\n",
    "    lower_y_ax = post_min - [1, 1, 1, 1, 0.2, 0.05]\n",
    "\n",
    "    time = np.arange(x_nn.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        ci_upper = means[:, i] + stds[:, i]\n",
    "        ci_lower = means[:, i] - stds[:, i]\n",
    "        ax.plot(time, means[:, i], color=NEURAL_COLOR, label='Post. mean')\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=0.6, linewidth=0, label='Post. std. deviation')\n",
    "\n",
    "        if ground_truths is not None:\n",
    "            ax.plot(time, ground_truths[:, i], color='black', linestyle='dashed', label='True Dynamic', lw=2)\n",
    "        sns.despine(ax=ax)\n",
    "\n",
    "        # ax.set_xlabel('Trial', fontsize=18)\n",
    "        # ax.set_ylabel('Parameter value ({})'.format(par_names[i]), fontsize=18)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=18)\n",
    "            ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax.set_ylim(lower_y_ax[i], upper_y_ax[i])\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "        # vertical bars\n",
    "        for idx in np.arange(799, 2400, 800):\n",
    "            if idx == 799:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        for idx in np.arange(99, 3100, 100):\n",
    "            if idx == 99:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "\n",
    "        # horizontal fast-dm params\n",
    "        if i <= 3:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[6], fast_dm_params[i] + fast_dm_params[6], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "        elif i == 4:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "        else:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[7]/2, fast_dm_params[i] + fast_dm_params[7]/2, color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/param_dynamic_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(N_SUBS):\n",
    "    plot_dynamic_posteriors(post_theta_t[:, sub, :, :], fast_dm_params[sub], PARAM_LABELS, PARAM_NAMES)\n",
    "    print(\"Sub {} is finished\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute means and stds for neural and fast-dm parameters\n",
    "neural_means = post_theta_t.mean(axis=0).mean(axis=0)\n",
    "neural_stds = post_theta_t.mean(axis=0).std(axis=0)\n",
    "\n",
    "fast_dm_means = fast_dm_params.mean(axis=0)\n",
    "fast_dm_sd = fast_dm_params.std(axis=0)\n",
    "\n",
    "post_max = np.array(neural_means).max(axis=0).max()\n",
    "upper_y_ax = post_max + 1\n",
    "\n",
    "sigma_factors = [1]\n",
    "alphas = [0.6]\n",
    "\n",
    "time = np.arange(N_OBS)\n",
    "f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.plot(time, neural_means[:, i], color=NEURAL_COLOR, label='Average post. mean')\n",
    "    for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "        ci_upper = neural_means[:, i] + sigma_factor * neural_stds[:, i]\n",
    "        ci_lower = neural_means[:, i] - sigma_factor * neural_stds[:, i]\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=alpha, linewidth=0, label='Std. deviation post. mean')\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_xlabel('Trial', fontsize=18)\n",
    "        ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "    ax.set_title(PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=20)\n",
    "    ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    # if i < 4:\n",
    "    #     ax.set_ylim(0, upper_y_ax)\n",
    "    # else:\n",
    "    #     ax.set_ylim(0)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # vertical bars\n",
    "    for idx in np.arange(799, 2400, 800):\n",
    "        if idx == 799:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "    for idx in np.arange(99, 3100, 100):\n",
    "        if idx == 99:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "    # horizontal fast-dm params\n",
    "    ax.plot(time, np.repeat(fast_dm_means[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Average Fast-dm estimate', lw=2.5)\n",
    "    ax.fill_between(time, fast_dm_means[i] - fast_dm_sd[i], fast_dm_means[i] + fast_dm_sd[i], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Std. deviation Fast-dm estimate')\n",
    "\n",
    "    f.subplots_adjust(hspace=0.5)\n",
    "    if i == 0:\n",
    "        f.legend(fontsize=16, loc='center', \n",
    "                    bbox_to_anchor=(0.5, -0.05),fancybox=False, shadow=False, ncol=4)\n",
    "\n",
    "f.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
