{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys, os, re\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import gamma, beta\n",
    "import talib\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src\")\n",
    "from helpers import build_distance_matrix\n",
    "from macro_models import batched_gaussian_process\n",
    "from priors import diffusion_prior, length_scale_prior\n",
    "from micro_models import dynamic_batch_diffusion, fast_dm_simulate, diffusion_trial\n",
    "from networks import DynamicGaussianNetworkJoint\n",
    "from context import generate_design_matrix\n",
    "from transformations import unscale_z, scale_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('CPU')\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T          = 3200\n",
    "N_PARAMS   = 6\n",
    "DIST_MAT   = build_distance_matrix(T)\n",
    "# AMPLITUDES = [0.15, 0.15, 0.15, 0.15, 0.1, 0.05]\n",
    "AMPLITUDES = [0.1, 0.1, 0.1, 0.1, 0.05, 0.05]\n",
    "\n",
    "N_SAMPLES  = 500\n",
    "N_SIM      = 100\n",
    "BATCH_SIZE = 16\n",
    "TEST_SIZE  = 10\n",
    "N_CHUNKS   = 8000\n",
    "EPOCHS     = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN = [5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
    "MACRO_STD  = [2.8, 2.8, 2.8, 2.8, 2.8, 2.8]\n",
    "MICRO_MEAN = [1.3, 1.3, 1.3, 1.3, 1.3, 0.3]\n",
    "MICRO_STD  = [1.0, 1.0, 1.0, 1.0, 0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPIRIC_COLOR    = '#1F1F1F'\n",
    "NEURAL_COLOR     = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'\n",
    "\n",
    "PARAM_LABELS = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "PARAM_NAMES  = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fun(batch_size):\n",
    "    repeat = True\n",
    "    while repeat:\n",
    "        theta0 = diffusion_prior(batch_size, n_cond=N_PARAMS-2)\n",
    "        eta = length_scale_prior(batch_size, N_PARAMS)\n",
    "        theta_t = batched_gaussian_process(theta0, DIST_MAT, eta, amplitudes=AMPLITUDES)\n",
    "        if min(theta_t[:, :, 4].flatten()) > 0.059:\n",
    "            repeat = False\n",
    "            \n",
    "    context = generate_design_matrix(batch_size, T)\n",
    "\n",
    "    rt = dynamic_batch_diffusion(theta_t, context)\n",
    "    x = np.concatenate((rt, to_categorical(context[:, :, np.newaxis])), axis=-1)\n",
    "\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    theta_t_z = scale_z(theta_t, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# eta_z, theta_t_z, x = generator_fun(16)\n",
    "\n",
    "# print(eta_z.shape)\n",
    "# print(theta_t_z.shape)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "# time = np.arange(1, theta_t_z.shape[1]+1)\n",
    "# for i, ax in enumerate(axarr.flat):\n",
    "#     ax.plot(time, theta_t_z[1, :, i], label='True', color='black', linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presimulate_data():\n",
    "    eta_z, theta_t_z, x = generator_fun(BATCH_SIZE)\n",
    "    np.save(f'../data/offline_data_new_2/data/x_{n}.npy', x)\n",
    "    np.save(f'../data/offline_data_new_2/parameters/eta/eta_params_{n}.npy', eta_z)\n",
    "    np.save(f'../data/offline_data_new_2/parameters/theta/theta_params_{n}.npy', theta_t_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presimulate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkLoader:\n",
    "    def __init__(self, path_to_data):\n",
    "        self.path_to_data = path_to_data\n",
    "        self.data_list = sorted(os.listdir(os.path.join(path_to_data, 'data')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.eta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/eta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.theta_list = sorted(os.listdir(os.path.join(path_to_data, 'parameters/theta')), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.num_batches = len(self.data_list)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_index < self.num_batches:\n",
    "            self.current_index += 1\n",
    "            idx = self.indices[self.current_index -1]\n",
    "            batch_x = np.load(os.path.join(self.path_to_data, 'data', self.data_list[idx]))\n",
    "            batch_eta_params = np.load(os.path.join(self.path_to_data, 'parameters/eta', self.eta_list[idx]))\n",
    "            batch_theta_params = np.load(os.path.join(self.path_to_data, 'parameters/theta', self.theta_list[idx]))\n",
    "            return batch_eta_params, batch_theta_params, batch_x\n",
    "        self.indices = list(range(len(self.data_list)))\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ChunkLoader('../data/offline_data_new_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_lstm_units' : 512, \n",
    "    'embedding_gru_units': 512,\n",
    "    'embedding_dense_args': dict(units=256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'posterior_dense_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'n_micro_params': 6,\n",
    "    'n_macro_params': 6\n",
    "}\n",
    "network = DynamicGaussianNetworkJoint(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 8000\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=steps_per_epoch*EPOCHS)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step, batch in enumerate(generator):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            eta_z, theta_t_z, data = batch\n",
    "\n",
    "            idx = np.random.choice(np.arange(0, 16), size=8, replace=False, p=None)\n",
    "\n",
    "            # Forward pass\n",
    "            posterior = network(data[idx])\n",
    "\n",
    "            # loss computation\n",
    "            loss = nll(tf.concat([tf.stack([eta_z[idx]] * T, axis=1), theta_t_z[idx]], axis=-1), posterior)\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=loader.num_batches, desc=f'Training Epoch {ep}') as p_bar:\n",
    "        loss_ep = epoch_trainer(loader, network, optimizer, steps_per_epoch, p_bar)\n",
    "        losses.append(loss_ep)\n",
    "    network.save_weights('../trained_networks/gpddm_3200_joint_offline_cosineDecay_newData8000_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.save_weights('../trained_networks/gpddm_3200_joint_offline_cosineDecay_newData8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.load_weights('../trained_networks/gpddm_3200_joint_offline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_test, theta_t_z_test, x_test = generator_fun(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z_pred, theta_t_z_pred = network.sample_n(x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_test = unscale_z(eta_z_test, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_test = unscale_z(theta_t_z_test, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "eta_pred = unscale_z(eta_z_pred, MACRO_MEAN, MACRO_STD)\n",
    "theta_t_pred = unscale_z(theta_t_z_pred, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t_pred_mean = theta_t_pred.numpy().mean(axis=0)\n",
    "theta_t_pred_std = theta_t_pred.numpy().std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(TEST_SIZE, 6, figsize=(25, 30))\n",
    "std_mul = 1\n",
    "time = np.arange(1, theta_t_test.shape[1]+1)\n",
    "for j in range(TEST_SIZE):\n",
    "    for i in range(6):\n",
    "        ax = axarr[j, i]\n",
    "        ax.plot(time, theta_t_test[j, :, i], label='True', color='black', linestyle='dashed')\n",
    "        ax.plot(time, theta_t_pred_mean[j, :, i], label='Pred', lw=2, color='#8c6eb5')\n",
    "        ax.fill_between(time, \n",
    "                        theta_t_pred_mean[j, :, i] + std_mul * theta_t_pred_std[j, :, i], \n",
    "                        theta_t_pred_mean[j, :, i] - std_mul * theta_t_pred_std[j, :, i], color='#8c6eb5', alpha=0.3)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fit on empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights('../trained_networks/gpddm_3200_joint_offline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('../data/data_lexical_decision.csv', sep=',', header=0)\n",
    "\n",
    "# prepare data for fitting\n",
    "ids = np.unique(data.id)\n",
    "N_SUBS = len(ids)\n",
    "\n",
    "# negative rts for error responses\n",
    "data.rt.loc[data.acc == 0] = -data.rt.loc[data.acc == 0]\n",
    "\n",
    "# iterate over subjects\n",
    "x_nn = np.zeros((len(ids), T, 5))\n",
    "\n",
    "for id in ids:\n",
    "    person_data = data[data.id == id]\n",
    "    rt = np.array([person_data.rt])[:, :, np.newaxis]\n",
    "    stim_type = np.array([person_data.stim_type])[:, :, np.newaxis] - 1 \n",
    "    context = to_categorical(stim_type)\n",
    "    x_nn[id-1] = tf.concat((rt, context), axis=-1)\n",
    "\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "post_eta_z = np.zeros((N_SAMPLES, N_SUBS, T, N_PARAMS))\n",
    "post_theta_t_z = np.zeros((N_SAMPLES, N_SUBS, T, N_PARAMS))\n",
    "for i in range(len(ids)):\n",
    "    post_eta_z[:, i:i+1, :], post_theta_t_z[:, i:i+1, :, :] = network.sample_n(x_nn[i:i+1], N_SAMPLES)\n",
    "    print(\"Sub nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fast-dm parameter estimates\n",
    "fast_dm_params = pd.read_csv('../data/parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "fast_dm_params = fast_dm_params.to_numpy()[:, 1:]\n",
    "fast_dm_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with fast_dm for all subjects\n",
    "pred_rt_fast_dm = np.zeros((N_SUBS, T))\n",
    "for i in range(N_SUBS):\n",
    "    context = data.stim_type.loc[data.id == i+1].to_numpy() - 1\n",
    "    pred_rt_fast_dm[i] = fast_dm_simulate(fast_dm_params[i], context)\n",
    "    \n",
    "pred_rt_fast_dm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check(emp_data, post_theta_t, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data.stim_type.values - 1\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data.rt.values)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/n_sim, dtype=np.int32)\n",
    "    theta = post_theta_t[idx]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta[sim, t, context[t]], theta[sim, t, 4], theta[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt, emp_rt, sma_emp_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with neural for all subjects\n",
    "pred_rt_neural = np.zeros((N_SUBS, N_SIM, T))\n",
    "sma_pred_rt_neural = np.zeros((N_SUBS, N_SIM, T))\n",
    "\n",
    "pred_rt_quantiles = np.zeros((N_SUBS, 2, T))\n",
    "pred_rt_medians = np.zeros((N_SUBS, T))\n",
    "\n",
    "emp_rt =  np.zeros((N_SUBS, T))\n",
    "sma_emp_rt =  np.zeros((N_SUBS, T))\n",
    "\n",
    "for sub in range(N_SUBS):\n",
    "    # predict RTs\n",
    "    person_data = data[data.id == sub+1]\n",
    "    pred_rt_neural[sub], sma_pred_rt_neural[sub], emp_rt[sub], sma_emp_rt[sub] = pr_check(person_data, post_theta_t[:, sub, :, :], N_SIM)\n",
    "    # compute RT quantiles\n",
    "    pred_rt_quantiles[sub] = np.quantile(sma_pred_rt_neural[sub], [0.025, 0.975], axis=0)\n",
    "    pred_rt_medians[sub] = np.median(sma_pred_rt_neural[sub], axis=0)\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=800\n",
    "emp_data_horizon = x_nn[:, :T-horizon, :]\n",
    "\n",
    "# inference on restircted data\n",
    "post_eta_z_horizon = np.zeros((N_SAMPLES, N_SUBS, T-horizon, N_PARAMS))\n",
    "post_theta_t_z_horizon = np.zeros((N_SAMPLES, N_SUBS, T-horizon, N_PARAMS))\n",
    "for i in range(len(ids)):\n",
    "    post_eta_z_horizon[:, i:i+1, :, :], post_theta_t_z_horizon[:, i:i+1, :, :] = network.sample_n(emp_data_horizon[i:i+1], N_SAMPLES)\n",
    "    print(\"Sub nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_last = unscale_z(post_eta_z_horizon[:, :, -1, :], MACRO_MEAN, MACRO_STD)\n",
    "post_theta_last = unscale_z(post_theta_t_z_horizon[:, :, -1, :], MICRO_MEAN,  MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/N_SIM, dtype=np.int32)\n",
    "post_eta_last_select = post_eta_last[idx]\n",
    "post_theta_last_select = post_theta_last[idx]\n",
    "post_eta_last_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic parameters and simulate RTs\n",
    "pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "sma_pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "\n",
    "context = x_nn[:, :, 1:].argmax(axis=2)[:, T-horizon:]\n",
    "dist_mat_horizon = build_distance_matrix(horizon)\n",
    "for sub in range(N_SUBS):\n",
    "    for i in range(N_SIM):\n",
    "        pred_theta_t = batched_gaussian_process(post_theta_last_select[i, sub:sub+1], dist_mat_horizon, post_eta_last_select[i, sub:sub+1], amplitudes=AMPLITUDES)\n",
    "        pred_rt_horizon[i, sub:sub+1] = np.abs(dynamic_batch_diffusion(pred_theta_t, context[sub:sub+1]).astype(np.float32))\n",
    "        sma_pred_rt_horizon[i, sub, :, 0] = talib.SMA(pred_rt_horizon[i, sub, :, 0], timeperiod=5)\n",
    "\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_horizon_medians = np.median(sma_pred_rt_horizon, axis=0)\n",
    "pred_rt_horizon_quantiles = np.quantile(sma_pred_rt_horizon, [0.025, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../saved_arrays/gpddm_post_eta_all_sub.npy\", post_eta)\n",
    "# np.save(\"../saved_arrays/gpddm_post_theta_t_all_sub.npy\", post_theta_t)\n",
    "# np.save(\"../saved_arrays/gpddm_fast_dm_params.npy\", fast_dm_params)\n",
    "# np.save(\"../saved_arrays/gpddm_pred_rt_fast_dm.npy\", pred_rt_fast_dm)\n",
    "\n",
    "# np.save(\"../saved_arrays/gpddm_pred_rt_neural\", pred_rt_neural)\n",
    "# np.save(\"../saved_arrays/gpddm_sma_pred_rt_neural\", sma_pred_rt_neural)\n",
    "# np.save(\"../saved_arrays/gpddm_pred_rt_quantiles\", pred_rt_quantiles)\n",
    "# np.save(\"../saved_arrays/gpddm_pred_rt_medians\", pred_rt_medians)\n",
    "# np.save(\"../saved_arrays/gpddm_emp_rt\", emp_rt)\n",
    "# np.save(\"../saved_arrays/gpddm_sma_emp_rt\", sma_emp_rt)\n",
    "\n",
    "# np.save('../saved_arrays/gpddm_pred_rt_horizon_medians.npy', pred_rt_horizon_medians)\n",
    "# np.save('../saved_arrays/gpddm_pred_rt_horizon_quantiles.npy', pred_rt_horizon_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_eta = np.load(\"../saved_arrays/gpddm_post_eta_all_sub.npy\")\n",
    "# post_theta_t = np.load(\"../saved_arrays/gpddm_post_theta_t_all_sub.npy\")\n",
    "# fast_dm_params = np.load(\"../saved_arrays/gpddm_fast_dm_params.npy\")\n",
    "# pred_rt_fast_dm = np.load(\"../saved_arrays/gpddm_pred_rt_fast_dm.npy\")\n",
    "\n",
    "# pred_rt_neural = np.load(\"../saved_arrays/gpddm_pred_rt_neural.npy\")\n",
    "# sma_pred_rt_neural = np.load(\"../saved_arrays/gpddm_sma_pred_rt_neural.npy\")\n",
    "# pred_rt_quantiles = np.load(\"../saved_arrays/gpddm_pred_rt_quantiles.npy\")\n",
    "# pred_rt_medians = np.load(\"../saved_arrays/gpddm_pred_rt_medians.npy\")\n",
    "# emp_rt = np.load(\"../saved_arrays/gpddm_emp_rt.npy\")\n",
    "# sma_emp_rt = np.load(\"../saved_arrays/gpddm_sma_emp_rt.npy\")\n",
    "\n",
    "# pred_rt_horizon_medians = np.load('../saved_arrays/gpddm_pred_rt_horizon_medians.npy')\n",
    "# pred_rt_horizon_quantiles = np.load('../saved_arrays/gpddm_pred_rt_horizon_quantiles.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderLegend2(ax=None,order=None,unique=False):\n",
    "    if ax is None: ax=plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0])) # sort both labels and handles by labels\n",
    "    if order is not None: # Sort according to a given list (not necessarily complete)\n",
    "        keys=dict(zip(order,range(len(order))))\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t,keys=keys: keys.get(t[0],np.inf)))\n",
    "    if unique:  labels, handles= zip(*unique_everseen(zip(labels,handles), key = labels)) # Keep only the first of each handle\n",
    "    ax.legend(handles, labels,\n",
    "              fontsize=16, loc='upper right')\n",
    "    return(handles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "horizon = 800\n",
    "for sub in range(N_SUBS):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                        gridspec_kw={'width_ratios': [6, 1]})\n",
    "    axrr = ax.flat\n",
    "    # plot empiric and predicted response times series\n",
    "    time = np.arange(T) \n",
    "\n",
    "    axrr[0].plot(time, sma_emp_rt[sub], color=EMPIRIC_COLOR, lw=1.4, alpha=0.8, label='SMA5: Empiric')\n",
    "    axrr[0].plot(time[:T-horizon], pred_rt_medians[sub, :T-horizon], color=NEURAL_COLOR, lw=1.4, label='SMA5: Post. re-simulation median', alpha=0.8)\n",
    "    axrr[0].plot(time[T-horizon:], pred_rt_horizon_medians[sub], color=\"#b35032\", lw=1.4, label='SMA5: Multi-horizon predictive median', alpha=0.6)\n",
    "    axrr[0].fill_between(time[T-horizon:], pred_rt_horizon_quantiles[0, sub, :, 0], pred_rt_horizon_quantiles[1, sub, :, 0], color=\"#b35032\", linewidth=0, alpha=0.4, label='Multi-horizon predictive 95% CI')\n",
    "    axrr[0].fill_between(time[:T-horizon], pred_rt_quantiles[sub, 0, :T-horizon], pred_rt_quantiles[sub, 1, :T-horizon], color=NEURAL_COLOR, linewidth=0, alpha=0.5, label='Post. re-simulation 95% CI')\n",
    "    \n",
    "    for idx in np.argwhere(person_data.session.diff().values == 1):\n",
    "        if idx == 800:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    for idx in np.argwhere(person_data.block.diff().values == 1):\n",
    "        if idx == 100:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    sns.despine(ax=axrr[0])\n",
    "    axrr[0].set_ylabel('RT(s)', fontsize=18, rotation=0, labelpad=40)\n",
    "    axrr[0].set_xlabel('\\nTrial', fontsize=18)\n",
    "    axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    f.legend(fontsize=16, loc='center', \n",
    "            bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "    axrr[0].grid(False)\n",
    "    axrr[0].set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "    # plot empiric and predicted response time dist\n",
    "    plt.setp(ax, ylim=(0, 1.5))\n",
    "    sns.histplot(y=np.abs(emp_rt[sub]), fill=EMPIRIC_COLOR, color=EMPIRIC_COLOR, alpha=0.8, label=\"Empiric\", ax=axrr[1], stat=\"density\", bins=250, linewidth=0)#062759\n",
    "    sns.kdeplot(y=np.abs(pred_rt_fast_dm[sub]), fill= COMPARISON_COLOR, color=COMPARISON_COLOR, alpha=0.3, label=\"Fast-dm\", ax=axrr[1], linewidth=3.5)#598f70\n",
    "    sns.kdeplot(y=pred_rt_neural[sub].flatten(), fill=NEURAL_COLOR, color=NEURAL_COLOR, alpha=0.3, label=\"Dynamic DDM\", ax=axrr[1], linewidth=3.5)\n",
    "\n",
    "    axrr[1].legend(fontsize=16)\n",
    "    axrr[1].set_xlabel('', fontsize=18)\n",
    "    axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    axrr[1].set_yticklabels('')\n",
    "    axrr[1].set_xticklabels('')\n",
    "    axrr[1].xaxis.set_ticks([])\n",
    "    axrr[1].yaxis.set_ticks([])\n",
    "    axrr[1].get_xaxis().set_visible(False)\n",
    "    for line in axrr[1].get_lines():\n",
    "        line.set_alpha(1)\n",
    "    sns.despine(ax=axrr[1], bottom=True)\n",
    "\n",
    "    axrr[0].annotate('Re-simulation',\n",
    "                xy=(0.38, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].annotate('Prediction',\n",
    "                xy=(0.84, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].tick_params(length=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.05)\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/gpddm_rt_time_series_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors(dynamic_posterior, fast_dm_params, par_labels, par_names, \n",
    "                            ground_truths=None):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "        \n",
    "    means = dynamic_posterior.mean(axis=0)\n",
    "    # quantiles = np.quantile(dynamic_posterior, [0.025, 0.975], axis=0)\n",
    "    stds = dynamic_posterior.std(axis=0)\n",
    "    \n",
    "    post_max = np.array(means).max(axis=0)\n",
    "    post_min = np.array(means).min(axis=0)\n",
    "    upper_y_ax = post_max + [1, 1, 1, 1, 0.2, 0.05]\n",
    "    lower_y_ax = post_min - [1, 1, 1, 1, 0.2, 0.05]\n",
    "\n",
    "    time = np.arange(x_nn.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        ci_upper = means[:, i] + stds[:, i]\n",
    "        ci_lower = means[:, i] - stds[:, i]\n",
    "        ax.plot(time, means[:, i], color=NEURAL_COLOR, label='Post. mean')\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=0.6, linewidth=0, label='Post. std. deviation')\n",
    "\n",
    "        if ground_truths is not None:\n",
    "            ax.plot(time, ground_truths[:, i], color='black', linestyle='dashed', label='True Dynamic', lw=2)\n",
    "        sns.despine(ax=ax)\n",
    "\n",
    "        # ax.set_xlabel('Trial', fontsize=18)\n",
    "        # ax.set_ylabel('Parameter value ({})'.format(par_names[i]), fontsize=18)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=18)\n",
    "            ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax.set_ylim(lower_y_ax[i], upper_y_ax[i])\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "        # vertical bars\n",
    "        for idx in np.arange(799, 2400, 800):\n",
    "            if idx == 799:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        for idx in np.arange(99, 3100, 100):\n",
    "            if idx == 99:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "\n",
    "        # horizontal fast-dm params\n",
    "        if i <= 3:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[6], fast_dm_params[i] + fast_dm_params[6], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "        elif i == 4:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "        else:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[7]/2, fast_dm_params[i] + fast_dm_params[7]/2, color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/gpddm_param_dynamic_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(N_SUBS):\n",
    "    plot_dynamic_posteriors(post_theta_t[:, sub, :, :], fast_dm_params[sub], PARAM_LABELS, PARAM_NAMES)\n",
    "    print(\"Sub {} is finished\".format(sub+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Average Parameter Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute means and stds for neural and fast-dm parameters\n",
    "neural_means = post_theta_t.mean(axis=0).mean(axis=0)\n",
    "neural_stds = post_theta_t.mean(axis=0).std(axis=0)\n",
    "\n",
    "fast_dm_means = fast_dm_params.mean(axis=0)\n",
    "fast_dm_sd = fast_dm_params.std(axis=0)\n",
    "\n",
    "post_max = np.array(neural_means).max(axis=0).max()\n",
    "upper_y_ax = post_max + 1\n",
    "\n",
    "sigma_factors = [1]\n",
    "alphas = [0.6]\n",
    "\n",
    "time = np.arange(T)\n",
    "f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.plot(time, neural_means[:, i], color=NEURAL_COLOR, label='Average post. mean')\n",
    "    for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "        ci_upper = neural_means[:, i] + sigma_factor * neural_stds[:, i]\n",
    "        ci_lower = neural_means[:, i] - sigma_factor * neural_stds[:, i]\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=alpha, linewidth=0, label='Std. deviation post. mean')\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_xlabel('Trial', fontsize=18)\n",
    "        ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "    ax.set_title(PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=20)\n",
    "    ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    # if i < 4:\n",
    "    #     ax.set_ylim(0, upper_y_ax)\n",
    "    # else:\n",
    "    #     ax.set_ylim(0)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # vertical bars\n",
    "    for idx in np.arange(799, 2400, 800):\n",
    "        if idx == 799:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "    for idx in np.arange(99, 3100, 100):\n",
    "        if idx == 99:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "    # horizontal fast-dm params\n",
    "    ax.plot(time, np.repeat(fast_dm_means[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Average Fast-dm estimate', lw=2.5)\n",
    "    ax.fill_between(time, fast_dm_means[i] - fast_dm_sd[i], fast_dm_means[i] + fast_dm_sd[i], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Std. deviation Fast-dm estimate')\n",
    "\n",
    "    f.subplots_adjust(hspace=0.5)\n",
    "    if i == 0:\n",
    "        f.legend(fontsize=16, loc='center', \n",
    "                    bbox_to_anchor=(0.5, -0.05),fancybox=False, shadow=False, ncol=4)\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(\"../plots/gpddm_average_param_dynamic.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check_simulation(emp_data, post_theta_t, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data[:, 1:].argmax(axis=1)\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data[:, 0], dtype=np.float64)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/n_sim, dtype=np.int32)\n",
    "    theta = post_theta_t[idx]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta[sim, t, context[t]], theta[sim, t, 4], theta[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt, emp_rt, sma_emp_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "true_eta_z, true_theta_t_z, true_data = generator_fun(batch_size)\n",
    "true_theta_t_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "post_eta_z = np.zeros((N_SAMPLES, batch_size, T, N_PARAMS))\n",
    "post_theta_t_z = np.zeros((N_SAMPLES, batch_size, T, N_PARAMS))\n",
    "for i in range(batch_size):\n",
    "    post_eta_z[:, i:i+1, :, :], post_theta_t_z[:, i:i+1, :, :] = network.sample_n(true_data[i:i+1], N_SAMPLES)\n",
    "    print(\"Sim nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEAN, MICRO_STD) \n",
    "\n",
    "true_eta = unscale_z(true_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "true_theta_t = unscale_z(true_theta_t_z, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../saved_arrays/true_data_simulation.npy\", true_data)\n",
    "np.save(\"../saved_arrays/post_eta_simulation.npy\", post_eta)\n",
    "np.save(\"../saved_arrays/post_theta_t_simulation.npy\", post_theta_t)\n",
    "np.save(\"../saved_arrays/true_eta_simulation.npy\", true_eta)\n",
    "np.save(\"../saved_arrays/true_theta_t_simulation.npy\", true_theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = np.load(\"../saved_arrays/true_data_simulation.npy\")\n",
    "post_eta = np.load(\"../saved_arrays/post_eta_simulation.npy\")\n",
    "post_theta_t = np.load(\"../saved_arrays/post_theta_t_simulation.npy\")\n",
    "true_eta = np.load(\"../saved_arrays/true_eta_simulation.npy\")\n",
    "true_theta_t = np.load(\"../saved_arrays/true_theta_t_simulation.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# horizon = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with neural for all subjects\n",
    "pred_rt_neural = np.zeros((batch_size, N_SIM, T))\n",
    "sma_pred_rt_neural = np.zeros((batch_size, N_SIM, T))\n",
    "\n",
    "pred_rt_quantiles = np.zeros((batch_size, 2, T))\n",
    "pred_rt_medians = np.zeros((batch_size, T))\n",
    "\n",
    "true_rt =  np.zeros((batch_size, T))\n",
    "sma_true_rt =  np.zeros((batch_size, T))\n",
    "\n",
    "for sim in range(batch_size):\n",
    "    # predict RTs\n",
    "    single_data = true_data[sim]\n",
    "    pred_rt_neural[sim], sma_pred_rt_neural[sim], true_rt[sim], sma_true_rt[sim] = pr_check_simulation(single_data, post_theta_t[:, sim, :, :], N_SIM)\n",
    "    # compute RT quantiles\n",
    "    pred_rt_quantiles[sim] = np.quantile(sma_pred_rt_neural[sim], [0.025, 0.975], axis=0)\n",
    "    pred_rt_medians[sim] = np.median(sma_pred_rt_neural[sim], axis=0)\n",
    "    print(\"Sim nr. {} is predicted\".format(sim+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=800\n",
    "true_data_horizon = true_data[:, :T-horizon, :]\n",
    "\n",
    "# inference on restircted data\n",
    "post_eta_z_horizon = np.zeros((N_SAMPLES, batch_size, T-horizon, N_PARAMS))\n",
    "post_theta_t_z_horizon = np.zeros((N_SAMPLES, batch_size, T-horizon, N_PARAMS))\n",
    "for i in range(batch_size):\n",
    "    post_eta_z_horizon[:, i:i+1, :, :], post_theta_t_z_horizon[:, i:i+1, :, :] = network.sample_n(true_data_horizon[i:i+1], N_SAMPLES)\n",
    "    print(\"Sim nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_last = unscale_z(post_eta_z_horizon[:, :, -1, :], MACRO_MEAN, MACRO_STD)\n",
    "post_theta_last = unscale_z(post_theta_t_z_horizon[:, :, -1, :], MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/N_SIM, dtype=np.int32)\n",
    "post_eta_last_select = post_eta_last[idx]\n",
    "post_theta_last_select = post_theta_last[idx]\n",
    "post_eta_last_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic parameters and simulate RTs\n",
    "pred_rt_horizon = np.zeros((N_SIM, batch_size, horizon, 1))\n",
    "sma_pred_rt_horizon = np.zeros((N_SIM, batch_size, horizon, 1))\n",
    "\n",
    "context = true_data[:, :, 1:].argmax(axis=2)[:, T-horizon:]\n",
    "dist_mat_horizon = build_distance_matrix(horizon)\n",
    "for sub in range(batch_size):\n",
    "    for i in range(N_SIM):\n",
    "        pred_theta_t = batched_gaussian_process(post_theta_last_select[i, sub:sub+1], dist_mat_horizon, post_eta_last_select[i, sub:sub+1], amplitudes=AMPLITUDES)\n",
    "        pred_rt_horizon[i, sub:sub+1] = np.abs(dynamic_batch_diffusion(pred_theta_t, context[sub:sub+1]).astype(np.float32))\n",
    "        sma_pred_rt_horizon[i, sub, :, 0] = talib.SMA(pred_rt_horizon[i, sub, :, 0], timeperiod=5)\n",
    "\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../saved_arrays/post_eta_last_means.npy\", post_eta_last_means)\n",
    "# np.save(\"../saved_arrays/post_theta_last_means.npy\", post_theta_last_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_eta_last_means = np.load(\"../saved_arrays/post_eta_last_means.npy\")\n",
    "# post_eta_last_means = np.load(\"../saved_arrays/post_theta_last_means.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_horizon_medians = np.median(sma_pred_rt_horizon, axis=0)\n",
    "pred_rt_horizon_quantiles = np.quantile(sma_pred_rt_horizon, [0.025, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../saved_arrays/pred_rt_neural_simulation.npy\", pred_rt_neural)\n",
    "# np.save(\"../saved_arrays/sma_pred_rt_neural_simulation.npy\", sma_pred_rt_neural)\n",
    "# np.save(\"../saved_arrays/pred_rt_quantiles_simulation.npy\", pred_rt_quantiles)\n",
    "# np.save(\"../saved_arrays/pred_rt_medians_simulation.npy\", pred_rt_medians)\n",
    "# np.save(\"../saved_arrays/true_rt_simulation.npy\", true_rt)\n",
    "# np.save(\"../saved_arrays/sma_true_rt_simulation.npy\", sma_true_rt)\n",
    "# np.save('../saved_arrays/pred_rt_horizon_medians_simulation.npy', pred_rt_horizon_medians)\n",
    "# np.save('../saved_arrays/pred_rt_horizon_quantiles_simulation.npy', pred_rt_horizon_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_neural = np.load(\"../saved_arrays/pred_rt_neural_simulation.npy\")\n",
    "sma_pred_rt_neural = np.load(\"../saved_arrays/sma_pred_rt_neural_simulation.npy\")\n",
    "pred_rt_quantiles = np.load(\"../saved_arrays/pred_rt_quantiles_simulation.npy\")\n",
    "pred_rt_medians = np.load(\"../saved_arrays/pred_rt_medians_simulation.npy\")\n",
    "true_rt = np.load(\"../saved_arrays/true_rt_simulation.npy\")\n",
    "sma_true_rt = np.load(\"../saved_arrays/sma_true_rt_simulation.npy\")\n",
    "pred_rt_horizon_medians = np.load('../saved_arrays/pred_rt_horizon_medians_simulation.npy')\n",
    "pred_rt_horizon_quantiles = np.load('../saved_arrays/pred_rt_horizon_quantiles_simulation.npy')\n",
    "\n",
    "\n",
    "post_eta = np.load(\"../saved_arrays/post_eta_simulation.npy\")\n",
    "post_theta_t = np.load(\"../saved_arrays/post_theta_t_simulation.npy\")\n",
    "true_eta = np.load(\"../saved_arrays/true_eta_simulation.npy\")\n",
    "true_theta_t = np.load(\"../saved_arrays/true_theta_t_simulation.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "horizon = 800\n",
    "for sub in range(batch_size):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                        gridspec_kw={'width_ratios': [6, 1]})\n",
    "    axrr = ax.flat\n",
    "    # plot empiric and predicted response times series\n",
    "    time = np.arange(T)\n",
    "    N_OBS = T\n",
    "\n",
    "    axrr[0].plot(time, sma_true_rt[sub], color=EMPIRIC_COLOR, lw=1.4, alpha=0.7, label='SMA5: Simulated')\n",
    "    axrr[0].plot(time[:N_OBS-horizon], pred_rt_medians[sub, :N_OBS-horizon], color=NEURAL_COLOR, lw=1.4, label='SMA5: Post. re-simulation median', alpha=0.8)\n",
    "    axrr[0].plot(time[N_OBS-horizon:], pred_rt_horizon_medians[sub], color=\"#b35032\", lw=1.4, label='SMA5: Multi-horizon predictive median', alpha=0.6)\n",
    "    axrr[0].fill_between(time[N_OBS-horizon:], pred_rt_horizon_quantiles[0, sub, :, 0], pred_rt_horizon_quantiles[1, sub, :, 0], color=\"#b35032\", linewidth=0, alpha=0.4, label='Multi-horizon predictive 95% CI')\n",
    "    axrr[0].fill_between(time[:N_OBS-horizon], pred_rt_quantiles[sub, 0, :N_OBS-horizon], pred_rt_quantiles[sub, 1, :N_OBS-horizon], color=NEURAL_COLOR, linewidth=0, alpha=0.5, label='Post. re-simulation 95% CI')\n",
    "    # for idx in np.argwhere(person_data.session.diff().values == 1):\n",
    "    #     if idx == 800:\n",
    "    #         axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    #     else:\n",
    "    #         axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    # for idx in np.argwhere(person_data.block.diff().values == 1):\n",
    "    #     if idx == 100:\n",
    "    #         axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    #     else:\n",
    "    #         axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    sns.despine(ax=axrr[0])\n",
    "    axrr[0].set_ylabel('RT(s)', fontsize=18, rotation=0, labelpad=40)\n",
    "    axrr[0].set_xlabel('\\nTrial', fontsize=18)\n",
    "    axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    f.legend(fontsize=16, loc='center', \n",
    "            bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "    \n",
    "    axrr[0].set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "    # plot empiric and predicted response time dist\n",
    "    plt.setp(ax, ylim=(0, np.nanmax(pred_rt_medians[sub])*1.5))\n",
    "    sns.histplot(y=np.abs(true_rt[sub]), fill=EMPIRIC_COLOR, color=EMPIRIC_COLOR, alpha=0.7, label=\"Simulated\", ax=axrr[1], stat=\"density\", bins=250, linewidth=0)\n",
    "    sns.kdeplot(y=pred_rt_neural[sub].flatten(), fill=NEURAL_COLOR, color=NEURAL_COLOR, alpha=0.3, label=\"Dynamic DDM\", ax=axrr[1], linewidth=3.5)\n",
    "\n",
    "    axrr[1].legend(fontsize=16)\n",
    "    axrr[1].set_xlabel('', fontsize=18)\n",
    "    axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    axrr[1].set_yticklabels('')\n",
    "    axrr[1].set_xticklabels('')\n",
    "    axrr[1].xaxis.set_ticks([])\n",
    "    axrr[1].yaxis.set_ticks([])\n",
    "    axrr[1].get_xaxis().set_visible(False)\n",
    "    for line in axrr[1].get_lines():\n",
    "        line.set_alpha(1)\n",
    "    sns.despine(ax=axrr[1], bottom=True)\n",
    "\n",
    "    axrr[0].annotate('Re-simulation',\n",
    "                xy=(0.38, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].annotate('Prediction',\n",
    "                xy=(0.84, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].tick_params(length=8)\n",
    "    axrr[0].grid(False)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.05)\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/rt_time_series_simulation_{}.pdf\".format(sub+1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors_simulation(dynamic_posterior, ground_truths, par_labels, par_names):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "        \n",
    "    means = dynamic_posterior.mean(axis=0)\n",
    "    std = dynamic_posterior.std(axis=0)\n",
    "    \n",
    "    post_max = np.array(means).max(axis=0)\n",
    "    post_min = np.array(means).min(axis=0)\n",
    "    upper_y_ax = post_max + [1, 1, 1, 1, 0.2, 0.05]\n",
    "    lower_y_ax = post_min - [1, 1, 1, 1, 0.2, 0.05]\n",
    "\n",
    "    sigma_factors = [1]\n",
    "    alphas = [0.5]\n",
    "\n",
    "    time = np.arange(dynamic_posterior.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        \n",
    "        ax.plot(time, means[:, i], color=NEURAL_COLOR, label='Post. mean', lw=1.5, alpha=0.8)\n",
    "        for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "            ci_upper = means[:, i] + sigma_factor * std[:, i]\n",
    "            ci_lower = means[:, i] - sigma_factor * std[:, i]\n",
    "            ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=alpha, linewidth=0, label='Post. std. deviation')\n",
    "            ax.plot(time, ground_truths[:, i], color=EMPIRIC_COLOR, label='True Dynamic', lw=2, alpha=0.8)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=18)\n",
    "            ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax.set_ylim(lower_y_ax[i], upper_y_ax[i])\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/param_dynamic_simulation_{}.pdf\".format(sim+1), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim in range(BATCH_SIZE):\n",
    "    plot_dynamic_posteriors_simulation(post_theta_t[:, sim, :, :], true_theta_t[sim], PARAM_LABELS, PARAM_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_NAMES  = [r'$\\eta_{v_1}$', r'$\\eta_{v_2}$', r'$\\eta_{v_3}$', r'$\\eta_{v_4}$', r'$\\eta_{a}$', r'$\\eta_{\\tau}$']\n",
    "\n",
    "for sim in range(BATCH_SIZE):\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        sns.kdeplot(post_eta[:, sim, 2999, i], ax=ax, color='#852626', fill='#852626', alpha=0.5, label=\"Post.\", lw=2)\n",
    "        ax.axvline(true_eta[sim, i], label='True', color=\"#062759\", lw=2)\n",
    "\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Parameter value', fontsize=18)\n",
    "            ax.set_ylabel(\"Density\", fontsize=18)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\", fontsize=18)\n",
    "\n",
    "\n",
    "        ax.set_title('Eta ' + PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=20)\n",
    "        # ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "        # if i < 4:\n",
    "        #     ax.set_ylim(0, upper_y_ax)\n",
    "        # else:\n",
    "        #     ax.set_ylim(0)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        ax.grid(False)\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                    bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 500\n",
    "n_post_samples = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# generate data\n",
    "eta_z, theta_t_z, data = generator_fun(n_sim, N_OBS)\n",
    "print(eta_z.shape)\n",
    "print(theta_t_z.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
