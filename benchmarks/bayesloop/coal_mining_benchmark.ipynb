{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import sys\n",
    "\n",
    "from scipy.stats import gamma, beta, expon\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "from transformations import scale_z, unscale_z\n",
    "from networks import DynamicGaussianNetworkJoint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesloop as bl\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([5, 4, 1, 0, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, 1, 4,\n",
    "                 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0,\n",
    "                 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
    "                 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 3, 3, 0,\n",
    "                 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0])\n",
    "\n",
    "N_OBS = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN = beta.mean(1, 25)\n",
    "MACRO_STD = beta.std(1, 25)\n",
    "MICRO_MEAN = expon.mean(scale=2)\n",
    "MICRO_STD = expon.std(scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPIRIC_COLOR = '#1F1F1F'\n",
    "NEURAL_COLOR = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font type\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = bl.HyperStudy()\n",
    "S.load(data, timestamps=np.arange(1852, 1962))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_length = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low level model\n",
    "L = bl.observationModels.Poisson('accident_rate', bl.oint(0, 15, grid_length), prior=sympy.stats.Exponential('expon', 0.5))\n",
    "S.set(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high level model\n",
    "T = bl.transitionModels.GaussianRandomWalk('sigma', bl.oint(0, 1, grid_length), target='accident_rate', prior=sympy.stats.Beta(\"beta\", 1, 25))\n",
    "S.set(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "S.fit(forwardOnly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get micro posterior densities\n",
    "time = np.arange(1852, 1962)\n",
    "n_obs = time.shape[0]\n",
    "\n",
    "post_densities = np.zeros((n_obs, grid_length))\n",
    "for t in range(n_obs):\n",
    "    post_densities[t] = S.getParameterDistribution(time[t], \"accident_rate\")[1]\n",
    "\n",
    "post_means = S.getParameterMeanValues(\"accident_rate\")\n",
    "post_grid = S.getParameterDistribution(1852, \"accident_rate\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate posterior stds\n",
    "bl_post_std = np.zeros(n_obs)\n",
    "for i in range(n_obs):\n",
    "    center_grid = (post_grid - post_means[i])**2\n",
    "    bl_post_std[i] = np.sqrt(np.sum(post_densities[i] * center_grid) / np.sum(post_densities[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_prior_fun(batch_size, scale=1):\n",
    "    return default_rng().exponential(scale, size=(batch_size, 1)).astype(np.float32)\n",
    "\n",
    "def macro_prior_fun(batch_size, alpha=1., beta=25.):\n",
    "    return default_rng().beta(alpha, beta, size=(batch_size, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def random_walk(theta0, sigmas, T, lower_bound=0., upper_bound=8.):\n",
    "    B = theta0.shape[0]\n",
    "    D = theta0.shape[1]\n",
    "    theta_t = np.zeros((B, T, D))\n",
    "    theta_t[:, 0, :] = theta0\n",
    "    z = np.random.randn(B, T-1, D)\n",
    "    for t in range(1, T):\n",
    "        theta_t[:, t, :] = np.minimum(\n",
    "            np.maximum(theta_t[:, t-1, :] + sigmas * z[:, t-1, :], lower_bound), upper_bound)\n",
    "    return theta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_process(theta_t):\n",
    "    T = theta_t.shape[0]\n",
    "    x = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        x[t] = np.random.poisson(lam=theta_t[t])\n",
    "    return np.atleast_2d(x).T\n",
    "\n",
    "def batch_poisson_process(theta_t, diff_fun=poisson_process):\n",
    "    B, T = theta_t.shape[0], theta_t.shape[1]\n",
    "    x = np.zeros((B, T, 1))\n",
    "    for b in range(B):\n",
    "        x[b] = diff_fun(theta_t[b])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back transform of np.log1p(x) is np.expm1(x)\n",
    "def generator_fun(batch_size, T):\n",
    "    theta0 = micro_prior_fun(batch_size)\n",
    "    eta = macro_prior_fun(batch_size)\n",
    "    theta_t =  random_walk(theta0, eta, T)\n",
    "    x = batch_poisson_process(theta_t)\n",
    "\n",
    "    # standardize parameters\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    theta_t_z = scale_z(theta_t, MICRO_MEAN, MICRO_STD)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), np.log1p(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z, theta_t_z, x = generator_fun(32, 110)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            macro_params, micro_params, data = generator() \n",
    "\n",
    "            # Forward pass\n",
    "            posterior = network(data)\n",
    "\n",
    "            # loss computation\n",
    "            T = int(micro_params.shape[1])\n",
    "            loss = nll(tf.concat([tf.stack([macro_params] * T, axis=1), micro_params], axis=-1), posterior)\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_lstm_units' : 512,\n",
    "    'embedding_gru_units': 512,\n",
    "    'embedding_dense_args': dict(units=256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'posterior_dense_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'n_micro_params': 1,\n",
    "    'n_macro_params': 1\n",
    "}\n",
    "network = DynamicGaussianNetworkJoint(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = N_OBS\n",
    "batch_size = 32\n",
    "simulator = partial(generator_fun, T=T, batch_size=batch_size)\n",
    "epochs = 25\n",
    "steps_per_epoch = 1000\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.8,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for ep in range(1, epochs+1):\n",
    "#     with tqdm(total=steps_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "#         losses_ep = epoch_trainer(simulator, network, optimizer, steps_per_epoch, p_bar)\n",
    "#         losses.append(losses_ep)\n",
    "#     network.save_weights('trained_networks/poisson_process_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights(\"../../trained_networks/poisson_process_joint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = np.atleast_3d(np.log1p(data).astype(np.float32)).T\n",
    "log_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data\n",
    "eta, theta_t = network.sample_n(log_data[:1, :, :1], 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_post = theta_t[:, 0, :, 0]\n",
    "micro_post = unscale_z(micro_post, MICRO_MEAN, MICRO_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_post_mean = micro_post.numpy().mean(axis=0)\n",
    "micro_post_std = micro_post.numpy().std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(1852, 1962)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(time, micro_post_mean, alpha=0.9, color=NEURAL_COLOR)\n",
    "plt.fill_between(time, micro_post_mean+micro_post_std/2, micro_post_mean-micro_post_std/2, alpha=0.6, label='Neural', color=NEURAL_COLOR, edgecolor=\"none\")\n",
    "\n",
    "plt.plot(time, post_means, alpha=0.9, color=COMPARISON_COLOR)\n",
    "plt.fill_between(time, post_means+bl_post_std/2, post_means-bl_post_std/2, alpha=0.6, label='BayesLoop', color=COMPARISON_COLOR, edgecolor=\"none\")\n",
    "# plot of raw data\n",
    "plt.bar(time, data, align='center', facecolor=EMPIRIC_COLOR, alpha=0.6, label=\"Accident counts\")\n",
    "\n",
    "plt.ylabel('Accident rate', fontsize=28)\n",
    "plt.xlabel('Year', fontsize=28)\n",
    "plt.tick_params(axis='both', which='major', length=10, labelsize=24)\n",
    "\n",
    "plt.legend(fontsize=24)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../plots/plot_coal_mining_joint.pdf', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cogModel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
