{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "\n",
    "from scipy.stats import gamma, beta\n",
    "# import talib\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../src\")\n",
    "from networks_10092022 import DynamicGaussianNetworkJoint\n",
    "from priors import diffusion_prior, random_walk_prior\n",
    "from micro_models import simple_batch_diffusion, diffusion_trial\n",
    "from macro_models import random_walk_shared_var, random_walk\n",
    "from context import generate_design_matrix\n",
    "from transformations import scale_z, unscale_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SIM = 100\n",
    "N_OBS = 100\n",
    "N_SAMPLES = 4000\n",
    "N_PARAMS = 3\n",
    "\n",
    "PARAM_LABELS = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "PARAM_NAMES  = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']\n",
    "\n",
    "EMPIRIC_COLOR = '#1F1F1F'\n",
    "NEURAL_COLOR = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN  = beta(a=1, b=25).mean()\n",
    "MACRO_STD   = beta(a=1, b=25).std()\n",
    "MICRO_MEANS = [1.75, 1.7, 1] # calculated based on 10000 simulated theta_1:3200\n",
    "MICRO_STDS   = [1.5, 1.25, 1] # calculated based on 10000 simulated theta_1:3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load simulated data and ture params\n",
    "sim = pd.read_pickle('../../data/sim_data/static_dm_data_100.pkl')\n",
    "x_nn = sim['rt']\n",
    "micro_true = sim['theta']\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dynamic stan posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"/Users/lukas/Documents/github/dynamic_dm/benchmarks/stan/dynamic_stan_fits\"\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "files.sort(key=natural_keys)\n",
    "len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_stan_post = np.empty((N_SIM, N_OBS, N_SAMPLES, N_PARAMS))\n",
    "for i in range(len(files)):\n",
    "    post_samples = pd.read_csv('dynamic_stan_fits/' + files[i], index_col=False)\n",
    "    dynamic_stan_post[i] = post_samples.to_numpy()[:, 3:].reshape(100, 4000, 3)\n",
    "\n",
    "dynamic_stan_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std of posterior stds\n",
    "dynamic_stan_post_std_means = dynamic_stan_post.std(axis=2).mean(axis=0)\n",
    "dynamic_stan_post_std_stds = dynamic_stan_post.std(axis=2).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posterior means\n",
    "dynamic_stan_post_means = dynamic_stan_post.mean(axis=2)\n",
    "dynamic_stan_post_means.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_stan_abs_error = np.empty((N_SIM, N_OBS, N_PARAMS))\n",
    "for i in range(N_SIM):\n",
    "    dynamic_stan_abs_error[i] = np.abs(dynamic_stan_post_means[i] - micro_true[i])\n",
    "dynamic_stan_abs_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_stan_abs_error_mean = dynamic_stan_abs_error.mean(axis=0)\n",
    "dynamic_stan_abs_error_std = dynamic_stan_abs_error.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fun(batch_size, T, scale_micro_by_max=8., scale_macro=True):\n",
    "    theta = diffusion_prior(batch_size, n_cond=1)\n",
    "    eta = random_walk_prior(batch_size, 3)\n",
    "    theta_t = random_walk(theta, eta, T)\n",
    "    rt = simple_batch_diffusion(theta_t).astype(np.float32)\n",
    "\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    theta_t_z = scale_z(theta_t, MICRO_MEANS,  MICRO_STDS)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_z, theta_t_z, rt = generator_fun(32, N_OBS)\n",
    "theta_t_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            macro_params, micro_params, data = generator() \n",
    "\n",
    "            # Forward pass\n",
    "            posterior = network(data)\n",
    "\n",
    "            # loss computation\n",
    "            T = int(micro_params.shape[1])\n",
    "            loss = nll(tf.concat([tf.stack([macro_params] * T, axis=1), micro_params], axis=-1), posterior)\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_lstm_units' : 512,\n",
    "    'embedding_gru_units': 512,\n",
    "    'embedding_dense_args': dict(units=256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'posterior_dense_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'n_micro_params': 3,\n",
    "    'n_macro_params': 3\n",
    "}\n",
    "network = DynamicGaussianNetworkJoint(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "batch_size = 32\n",
    "simulator = partial(generator_fun, T=T, batch_size=batch_size)\n",
    "epochs = 100\n",
    "steps_per_epoch = 1000\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.8,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for ep in range(1, epochs+1):\n",
    "#     with tqdm(total=steps_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "#         losses_ep = epoch_trainer(simulator, network, optimizer, steps_per_epoch, p_bar)\n",
    "#         losses.append(losses_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.save_weights('trained_networks/1_drift_100_joint_21Sept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit dynamic dm to static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights(\"../../trained_networks/1_drift_100_joint_21Sept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "post_eta_z, post_theta_t_z = network.sample_n(x_nn, N_SAMPLES)\n",
    "post_theta_t_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD).numpy()\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEANS, MICRO_STDS).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and sd posterior sd's from dynamic model fits\n",
    "neural_post_std_means = post_theta_t.std(axis=0).mean(axis=0)\n",
    "neural_post_std_stds = post_theta_t.std(axis=0).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_post_means = post_theta_t.mean(axis=0)\n",
    "neural_post_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_abs_error = np.empty((N_SIM, N_OBS, N_PARAMS))\n",
    "for i in range(N_SIM):\n",
    "    neural_abs_error[i] = np.abs(neural_post_means[i] - micro_true[i])\n",
    "\n",
    "neural_abs_error_mean = np.mean(neural_abs_error, axis=0)\n",
    "neural_stan_abs_error_std = np.std(neural_abs_error, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"saved_arrays/neural_abs_error_mean_joint.npy\", neural_abs_error_mean)\n",
    "# np.save(\"saved_arrays/neural_stan_abs_error_std_joint.npy\", neural_stan_abs_error_std)\n",
    "\n",
    "# np.save(\"saved_arrays/dynamic_stan_abs_error_mean_joint.npy\", dynamic_stan_abs_error_mean)\n",
    "# np.save(\"saved_arrays/dynamic_stan_abs_error_std_joint.npy\", dynamic_stan_abs_error_std)\n",
    "\n",
    "# np.save(\"saved_arrays/neural_post_std_means_joint.npy\", neural_post_std_means)\n",
    "# np.save(\"saved_arrays/neural_post_std_stds_joint.npy\", neural_post_std_stds)\n",
    "\n",
    "# np.save(\"saved_arrays/dynamic_stan_post_std_means_joint.npy\", dynamic_stan_post_std_means)\n",
    "# np.save(\"saved_arrays/dynamic_stan_post_std_stds_joint.npy\", dynamic_stan_post_std_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_abs_error_mean = np.load(\"saved_arrays/neural_abs_error_mean_joint.npy\")\n",
    "neural_stan_abs_error_std = np.load(\"saved_arrays/neural_stan_abs_error_std_joint.npy\")\n",
    "\n",
    "dynamic_stan_abs_error_mean = np.load(\"saved_arrays/dynamic_stan_abs_error_mean_joint.npy\")\n",
    "dynamic_stan_abs_error_std = np.load(\"saved_arrays/dynamic_stan_abs_error_std_joint.npy\")\n",
    "\n",
    "neural_post_std_means = np.load(\"saved_arrays/neural_post_std_means_joint.npy\")\n",
    "neural_post_std_stds = np.load(\"saved_arrays/neural_post_std_stds_joint.npy\")\n",
    "\n",
    "dynamic_stan_post_std_means = np.load(\"saved_arrays/dynamic_stan_post_std_means_joint.npy\")\n",
    "dynamic_stan_post_std_stds = np.load(\"saved_arrays/dynamic_stan_post_std_stds_joint.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = ['Drift rate', 'Threshold', 'Non-decision time']\n",
    "param_names = [r'$v$', r'$a$', r'$\\tau$']\n",
    "font_size_large = 32\n",
    "font_size_small = 26\n",
    "time = np.arange(dynamic_stan_post_std_stds.shape[0])\n",
    "ALPHA = 0.6\n",
    "f, axarr = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    if i < 3:\n",
    "        # plot neural results\n",
    "        ax.plot(time, neural_abs_error_mean[:, i], alpha=0.9, color=NEURAL_COLOR)\n",
    "        ax.fill_between(time, neural_abs_error_mean[:, i] - neural_stan_abs_error_std[:, i]/2, neural_abs_error_mean[:, i] + neural_stan_abs_error_std[:, i]/2,\n",
    "                        alpha=ALPHA, label='Neural Dynamic DDM', color=NEURAL_COLOR)\n",
    "\n",
    "        # plot dynamic stan\n",
    "        ax.plot(time, dynamic_stan_abs_error_mean[:, i], alpha=0.9, color=COMPARISON_COLOR)\n",
    "        ax.fill_between(time, dynamic_stan_abs_error_mean[:, i] - dynamic_stan_abs_error_std[:, i]/2, dynamic_stan_abs_error_mean[:, i] + dynamic_stan_abs_error_std[:, i]/2,\n",
    "                        alpha=ALPHA, label='Stan Dynamic DDM', color=COMPARISON_COLOR)\n",
    "\n",
    "        ax.set_title(param_labels[i] + ' ({})'.format(param_names[i]), fontsize=font_size_large)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=font_size_large)\n",
    "            ax.set_ylabel(r'MAE $(\\bar{\\theta} - \\theta^*)$', fontsize=font_size_large)\n",
    "        if i == 2:\n",
    "            ax.legend(fontsize=font_size_small, loc='best', fancybox=False, shadow=False)\n",
    "\n",
    "    else:\n",
    "        # plot neural results\n",
    "        ax.plot(time, neural_post_std_means[:, i-3], alpha=0.9, color=NEURAL_COLOR)\n",
    "        ax.fill_between(time, neural_post_std_means[:, i-3]+neural_post_std_stds[:, i-3]/2, neural_post_std_means[:, i-3]-neural_post_std_stds[:, i-3]/2,\n",
    "                        alpha=ALPHA, label='Neural Dynamic DM', color=NEURAL_COLOR)\n",
    "\n",
    "        # plot dynamic stan\n",
    "        ax.plot(time, dynamic_stan_post_std_means[:, i-3], alpha=0.9, color=COMPARISON_COLOR)\n",
    "        ax.fill_between(time, dynamic_stan_post_std_means[:, i-3]+dynamic_stan_post_std_stds[:, i-3]/2, dynamic_stan_post_std_means[:, i-3]-dynamic_stan_post_std_stds[:, i-3]/2,\n",
    "                        alpha=ALPHA, label='Stan Dynamic DM', color=COMPARISON_COLOR)\n",
    "        \n",
    "        \n",
    "        if i == 3:\n",
    "            ax.set_xlabel('Trial', fontsize=font_size_large)\n",
    "            ax.set_ylabel('Post. std. deviation', fontsize=font_size_large)\n",
    "\n",
    "    if i == 0 or i == 3:\n",
    "        ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8])\n",
    "    elif i == 1 or i == 4:\n",
    "        ax.set_yticks([0.0, 0.2, 0.4, 0.6])\n",
    "    else:\n",
    "        ax.set_yticks([0.0, 0.1, 0.2])\n",
    "\n",
    "    ax.set_xticks([1, 25, 50, 75, 100])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=28, length=8)\n",
    "    # ax.grid(alpha=0.3)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../plots/plot_stan_benchmark_joint.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cogModel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
